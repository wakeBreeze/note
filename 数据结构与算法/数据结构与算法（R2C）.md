# ==数据结构==

# [九大数据结构](https://mp.weixin.qq.com/s/ZVwIUN-xf9FuxOFXW8H3Nw)

数据结构想必大家都不会陌生，对于一个成熟的程序员而言，熟悉和掌握数据结构和算法也是基本功之一。数据结构本身其实不过是数据按照特点关系进行存储或者组织的集合，特殊的结构在不同的应用场景中往往会带来不一样的处理效率。

常用的数据结构可根据数据访问的特点分为**线性结构**和**非线性结`**。

`线性结构`：包括常见的链表、栈、队列等，

`非线性结构`：包括树、图等。

数据结构种类繁多，本文将通过**图解的方式**对常用的数据结构进行理论上的介绍和讲解，以方便大家掌握常用数据结构的基本知识。

##  1 数组

数组可以说是最基本最常见的数据结构。数组一般用来存储`相同类型`的数据，可通过数组名和下标进行数据的访问和更新。数组中元素的存储是按照先后顺序进行的，同时在内存中也是按照这个顺序进行连续存放。数组相邻元素之间的内存地址的间隔一般就是数组数据类型的大小。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKhhVl0iaBRe7ibIoOs2mkZIr8y2Pe4MslT3SrwengScwadlRrBu9j2QUw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

##  2 链表

链表相较于数组，除了数据域，还增加了指针域用于构建链式的存储数据。链表中每一个节点都包含 ***此节点的数据*** 和 ***指向下一节点地址的指针*** 。由于是通过指针进行下一个数据元素的查找和访问，使得链表的自由度更高（不要求地址内存连续）。

这表现在对节点进行增加和删除时，只需要对上一节点的指针地址进行修改，而无需变动其它的节点。不过事物皆有两极，指针带来高自由度的同时，自然会牺牲数据查找的效率和多余空间的使用。

一般常见的是有头有尾的单链表，对指针域进行反向链接，还可以形成双向链表或者循环链表。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKSQuZiadmMbuvIlk9pHdtcHnRnm0zEfEa6aMVznfVsceGCL2Uhr6Bsow/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### 链表和数组对比

链表和数组在实际的使用过程中需要根据自身的优劣势进行选择。链表和数组的异同点也是面试中高频的考察点之一。这里对单链表和数组的区别进行了对比和总结。

![图片](https://mmbiz.qpic.cn/mmbiz_png/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKibiaBqW3ppH3FhrOvIKmEPTRyp6gQxqI36RsQld7ZzEMAIOBRkaVwLWQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

##  3 跳表

从上面的对比中可以看出，链表虽然通过增加指针域提升了自由度，但是却导致数据的查询效率恶化。特别是当链表长度很长的时候，对数据的查询还得从头依次查询，这样的效率会更低。跳表的产生就是为了解决链表过长的问题，通过增加链表的多级索引来加快原始链表的查询效率。这样的方式可以让查询的时间复杂度从O(n)提升至O(logn)。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCK6G5wwE7z1RfMqdNUypjjvq4DibFBgiayVwhtIDMR2fuy4XfqVJPtJzng/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

跳表通过增加的多级索引能够实现高效的动态插入和删除，其效率和红黑树和平衡二叉树不相上下。目前 redis 和 levelDB 都有用到跳表。

从上图可以看出，索引级的指针域除了指向下一个索引位置的指针，还有一个down指针指向低一级的链表位置，这样才能实现跳跃查询的目的。

##  4 栈

栈是一种比较简单的数据结构，常用一句话描述其特性，后进先出。栈本身是一个线性表，但是在这个表中只有一个口子允许数据的进出。这种模式可以参考腔肠动物...即进食和排泄都用一个口...

栈的常用操作包括入栈push和出栈pop，对应于数据的压入和压出。还有访问栈顶数据、判断栈是否为空和判断栈的大小等。由于栈后进先出的特性，常可以作为数据操作的临时容器，对数据的顺序进行调控，与其它数据结构相结合可获得许多灵活的处理。

![图片](https://mmbiz.qpic.cn/mmbiz_gif/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKGnHQrX7Z1xF4BncrEv8nUf8icSMK69bmp2mCDcDLY6kZ6fLwjq4IcTg/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)

##  5 队列

队列是栈的兄弟结构，与栈的后进先出相对应，队列是一种先进先出的数据结构。顾名思义，队列的数据存储是如同排队一般，先存入的数据先被压出。常与栈一同配合，可发挥最大的实力。

![图片](https://mmbiz.qpic.cn/mmbiz_gif/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKNibekBDiasFHnZZ76Cp8FqOk4fw9pV8NCBia1fe8PkBka7RIoyA20RibqA/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)

##  6 树

树作为一种树状的数据结构，其数据节点之间的关系也如大树一样，将有限个节点根据不同层次关系进行排列，从而形成数据与数据之间的父子关系。常见的树的表示形式更接近“倒挂的树”，因为它将根朝上，叶朝下。

树的数据存储在结点中，每个结点有零个或者多个子结点。没有父结点的结点在最顶端，成为根节点；没有非根结点有且只有一个父节点；每个非根节点又可以分为多个不相交的子树。

这意味着树是具备层次关系的，父子关系清晰，家庭血缘关系明朗；这也是树与图之间最主要的区别。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKNavoC9v9mnicvMRnTHJ1Zu3MkORhxuChe9gQOicYGSfHNLZQ53RKuKYA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

别看树好像很高级，其实可看作是链表的高配版。树的实现就是对链表的指针域进行了扩充，增加了多个地址指向子结点。同时将“链表”竖起来，从而凸显了结点之间的层次关系，更便于分析和理解。

树可以衍生出许多的结构，若将指针域设置为双指针，那么即可形成最常见的二叉树，即每个结点最多有两个子树的树结构。二叉树根据结点的排列和数量还可进一度划分为完全二叉树、满二叉树、平衡二叉树、红黑树等。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKfZB3ze2WZYvbyLVhVqpZzJ6JaS10OcLdW24ichbHhSzL8M1oQSV70zw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

> **完全二叉树**：除了最后一层结点，其它层的结点数都达到了最大值；同时最后一层的结点都是按照从左到右依次排布。

> **满二叉树**：除了最后一层，其它层的结点都有两个子结点。

#### 平衡二叉树

平衡二叉树又被称为AVL树，它是一棵二叉排序树，且具有以下性质：它是一棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。

> **二叉排序树**：是一棵空树，或者：若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值；若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值；它的左、右子树也分别为二叉排序树。

> **树的高度**：结点层次的最大值

> **平衡因子**：左子树高度 - 右子树高度

二叉排序树意味着二叉树中的数据是排好序的，顺序为左结点<根节点<右结点，这表明二叉排序树的中序遍历结果是有序的。（还不懂二叉树四种遍历方式[前序遍历（根左右）、中序遍历（左根右）、后序遍历（左右根）、层序遍历]的同学赶紧补习！）

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKXDYaRrlvtia2iacadRSIFEicxkO4unHOiaw0V9UE5gDwYDCrWpVawSu55A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

平衡二叉树的产生是为了解决二叉排序树在插入时发生线性排列的现象。由于二叉排序树本身为有序，当插入一个有序程度十分高的序列时，生成的二叉排序树会持续在某个方向的字数上插入数据，导致最终的二叉排序树会退化为链表，从而使得二叉树的查询和插入效率恶化。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKfaAibcB63OAwzlzkLufia2QF3rtlwT1psViaeiaIYO9mwMqnHrG4mmKsKA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



平衡二叉树的出现能够解决上述问题，但是在构造平衡二叉树时，却需要采用不同的调整方式，使得二叉树在插入数据后保持平衡。主要的四种调整方式有LL（左旋）、RR（右旋）、LR（先左旋再右旋）、RL（先右旋再左旋）。这里先给大家介绍下简单的单旋转操作，左旋和右旋。LR和RL本质上只是LL和RR的组合。

> 在插入一个结点后应该沿搜索路径将路径上的结点平衡因子进行修改，当平衡因子大于1时，就需要进行平衡化处理。从发生不平衡的结点起，沿刚才回溯的路径取直接下两层的结点，如果这三个结点在一条直线上，则采用**单旋转**进行平衡化，如果这三个结点位于一条折线上，则采用**双旋转**进行平衡化。

左旋：S为当前需要左旋的结点，E为当前结点的父节点。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKBGHvdV70GdaQuysibYfzDXP364jf2q1dk697fZNXPHZeia9WxMpr8kqg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

左旋的操作可以用一句话简单表示：将当前结点S的左孩子旋转为当前结点父结点E的右孩子，同时将父结点E旋转为当前结点S的左孩子。可用动画表示：

![图片](https://mmbiz.qpic.cn/mmbiz_gif/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKasKia4NJPZwkLmLZavnj40iaHqThkFkib891Z7TONfY9c3fka7hjJKQIw/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)

右旋：S为当前需要右旋的结点，E为当前结点的父节点。右单旋是左单旋的镜像旋转。

![图片](https://mmbiz.qpic.cn/mmbiz_gif/TdGLaSU675gwBawZm4AtKbyUUqEHRqCK3DUia4OX340ze9SzDdIicfz50xRiaHAKq0PCDiaPicWEqeGGEkYcxDic1ekQ/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)

右旋的操作同样可以用一句话简单表示：将当前结点S的左孩子E的右孩子旋转为当前结点S的左孩子，同时将当前结点S旋转为左孩子E的右孩子。可用动画表示：

![图片](https://mmbiz.qpic.cn/mmbiz_gif/TdGLaSU675gwBawZm4AtKbyUUqEHRqCK3DUia4OX340ze9SzDdIicfz50xRiaHAKq0PCDiaPicWEqeGGEkYcxDic1ekQ/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)

#### 红黑树

平衡二叉树（AVL）为了追求高度平衡，需要通过平衡处理使得左右子树的高度差必须小于等于1。高度平衡带来的好处是能够提供更高的搜索效率，其最坏的查找时间复杂度都是O(logN)。但是由于需要维持这份高度平衡，所付出的代价就是当对树种结点进行插入和删除时，需要经过多次旋转实现复衡。这导致AVL的插入和删除效率并不高。

为了解决这样的问题，能不能找一种结构能够兼顾搜索和插入删除的效率呢？这时候红黑树便申请出战了。

红黑树具有五个特性：

> 1. 每个结点要么是红的要么是黑的。
> 2. 根结点是黑的。
> 3. 每个叶结点（叶结点即指树尾端NIL指针或NULL结点）都是黑的。
> 4. 如果一个结点是红的，那么它的两个儿子都是黑的。
> 5. 对于任意结点而言，其到叶结点树尾端NIL指针的每条路径都包含相同数目的黑结点。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKWgj3kfnUh77iavcPOWMPTPsPHVicqc3xJEPKX3pBe6mQI3CzfLsY0BJA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)红黑树通过将结点进行红黑着色，使得原本高度平衡的树结构被稍微打乱，平衡程度降低。红黑树不追求完全平衡，只要求达到部分平衡。这是一种折中的方案，大大提高了结点删除和插入的效率。C++中的STL就常用到红黑树作为底层的数据结构。

#### 红黑树VS平衡二叉树

![图片](https://mmbiz.qpic.cn/mmbiz_png/TdGLaSU675gwBawZm4AtKbyUUqEHRqCK5hPCbpd8CzpmiaUWFT6F8ZLfClgnpEpz3TgnHhgc1nKh3TIaK5fK71Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

除了上面所提及的树结构，还有许多广泛应用在数据库、磁盘存储等场景下的树结构。比如B树、B+树等。这里就先不介绍了诶，下次在讲述相关存储原理的时候将会着重介绍。（其实是因为懒）

##  7 堆

了解完二叉树，再来理解堆就不是什么难事了。堆通常是一个可以被看做一棵树的数组对象。堆的具体实现一般不通过指针域，而是通过构建一个一维数组与二叉树的父子结点进行对应，因此堆总是一颗完全二叉树。

对于任意一个父节点的序号n来说（这里n从0算），它的子节点的序号一定是2n+1，2n+2，因此可以直接用数组来表示一个堆。

不仅如此，堆还有一个性质：堆中某个节点的值总是不大于或不小于其父节点的值。将根节点最大的堆叫做最大堆或大根堆，根节点最小的堆叫做最小堆或小根堆。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKPFK7qibDibUPQE1hmoXf3J2uhibD0IVqvOolw3XgtiaorEsyJDYpBby3jg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

堆常用来实现优先队列，在面试中经常考的问题都是与排序有关，比如堆排序、topK问题等。由于堆的根节点是序列中最大或者最小值，因而可以在建堆以及重建堆的过程中，筛选出数据序列中的极值，从而达到排序或者挑选topK值的目的。

##  8 散列表

散列表也叫哈希表，是一种通过键值对直接访问数据的结构。在初中，我们就学过一种能够将一个x值通过一个函数获得对应的一个y值的操作，叫做`映射`。散列表的实现原理正是映射的原理，通过设定的一个关键字和一个映射函数，就可以直接获得访问数据的地址，实现O(1)的数据访问效率。在映射的过程中，事先设定的函数就是一个映射表，也可以称作散列函数或者哈希函数。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKs2VE1D9tRL8XrqJCwiaLGY66PiagrRy2M6oLnB22pBtU5qZ6XricqnXeA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

散列表的实现最关键的就是散列函数的定义和选择。一般常用的有以下几种散列函数：

> **直接寻址法**：取关键字或关键字的某个线性函数值为散列地址。
>
> **数字分析法**：通过对数据的分析，发现数据中冲突较少的部分，并构造散列地址。例如同学们的学号，通常同一届学生的学号，其中前面的部分差别不太大，所以用后面的部分来构造散列地址。
>
> **平方取中法**：当无法确定关键字里哪几位的分布相对比较均匀时，可以先求出关键字的平方值，然后按需要取平方值的中间几位作为散列地址。这是因为：计算平方之后的中间几位和关键字中的每一位都相关，所以不同的关键字会以较高的概率产生不同的散列地址。
>
> **取随机数法**：使用一个随机函数，取关键字的随机值作为散列地址，这种方式通常用于关键字长度不同的场合。
>
> **除留取余法**：取关键字被某个不大于散列表的表长 n 的数 m 除后所得的余数 p 为散列地址。这种方式也可以在用过其他方法后再使用。该函数对 m 的选择很重要，一般取素数或者直接用 n。

确定好散列函数之后，通过某个`key`值的确会得到一个唯一的`value`地址。但是却会出现一些特殊情况。即通过不同的`key`值可能会访问到同一个地址，这个现象称之为冲突。

冲突在发生之后，当在对不同的`key`值进行操作时会使得造成相同地址的数据发生覆盖或者丢失，是非常危险的。所以在设计散列表往往还需要采用冲突解决的办法。

常用的冲突处理方式有很多，常用的包括以下几种：

> **开放地址法**（也叫开放寻址法）：实际上就是当需要存储值时，对Key哈希之后，发现这个地址已经有值了，这时该怎么办？不能放在这个地址，不然之前的映射会被覆盖。这时对计算出来的地址进行一个探测再哈希，比如往后移动一个地址，如果没人占用，就用这个地址。如果超过最大长度，则可以对总长度取余。这里移动的地址是产生冲突时的增列序量。
>
> **再哈希法**：在产生冲突之后，使用关键字的其他部分继续计算地址，如果还是有冲突，则继续使用其他部分再计算地址。这种方式的缺点是时间增加了。
>
> **链地址法**：链地址法其实就是对Key通过哈希之后落在同一个地址上的值，做一个链表。其实在很多高级语言的实现当中，也是使用这种方式处理冲突的。
>
> **公共溢出区**：这种方式是建立一个公共溢出区，当地址存在冲突时，把新的地址放在公共溢出区里。

目前比较常用的冲突解决方法是链地址法，一般可以通过数组和链表的结合达到冲突数据缓存的目的。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKtm3chgpF29leIl4TcNQhLR9JariaUuhicgDgBI6rB3WcIdhJ2Xf2ib90g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

左侧数组的每个成员包括一个指针，指向一个链表的头。每发生一个冲突的数据，就将该数据作为链表的节点链接到链表尾部。这样一来，就可以保证冲突的数据能够区分并顺利访问。

考虑到链表过长造成的问题，还可以使用红黑树替换链表进行冲突数据的处理操作，来提高散列表的查询稳定性。

##  9 图

图相较于上文的几个结构可能接触的不多，但是在实际的应用场景中却经常出现。比方说交通中的线路图，常见的思维导图都可以看作是图的具体表现形式。

图结构一般包括顶点和边，顶点通常用圆圈来表示，边就是这些圆圈之间的连线。边还可以根据顶点之间的关系设置不同的权重，默认权重相同皆为1。此外根据边的方向性，还可将图分为有向图和无向图。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKo1hPoa84A4tFXEP5dU99GHibHN5XwLsFp1wPMf3QZoZqXR5IF8PRibVw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

图结构用抽象的图线来表示十分简单，顶点和边之间的关系非常清晰明了。但是在具体的代码实现中，为了将各个顶点和边的关系存储下来，却不是一件易事。

#### 邻接矩阵

目前常用的图存储方式为邻接矩阵，通过所有顶点的二维矩阵来存储两个顶点之间是否相连，或者存储两顶点间的边权重。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKgbQHiaeI0ibicpTrzIXwvKOzwwUfxoJJ1ytFHWaQI6tGsEL3JiavurKUjw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

无向图的邻接矩阵是一个对称矩阵，是因为边不具有方向性，若能从此顶点能够到达彼顶点，那么彼顶点自然也能够达到此顶点。此外，由于顶点本身与本身相连没有意义，所以在邻接矩阵中对角线上皆为0。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKPEKEAhpnQfzsYXOhCtPbXiaF4x1kXoicexOVic8BXSC8Ex9CZBHnClaxw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

有向图由于边具有方向性，因此彼此顶点之间并不能相互达到，所以其邻接矩阵的对称性不再。

用邻接矩阵可以直接从二维关系中获得任意两个顶点的关系，可直接判断是否相连。但是在对矩阵进行存储时，却需要完整的一个二维数组。若图中顶点数过多，会导致二维数组的大小剧增，从而占用大量的内存空间。

而根据实际情况可以分析得，图中的顶点并不是任意两个顶点间都会相连，不是都需要对其边上权重进行存储。那么存储的邻接矩阵实际上会存在大量的0。虽然可以通过稀疏表示等方式对稀疏性高的矩阵进行关键信息的存储，但是却增加了图存储的复杂性。

因此，为了解决上述问题，一种可以只存储相连顶点关系的邻接表应运而生。

#### 邻接表

在邻接表中，图的每一个顶点都是一个链表的头节点，其后连接着该顶点能够直接达到的相邻顶点。相较于无向图，有向图的情况更为复杂，因此这里采用有向图进行实例分析。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKGjr7y7PpruuiaBuo6R6Q2JRYwKPVSLQ3R8wx8fj9YDZnJJb52TMZO7A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)在邻接表中，每一个顶点都对应着一条链表，链表中存储的是顶点能够达到的相邻顶点。存储的顺序可以按照顶点的编号顺序进行。比如上图中对于顶点B来说，其通过有向边可以到达顶点A和顶点E，那么其对应的邻接表中的顺序即B->A->E，其它顶点亦如此。

通过邻接表可以获得从某个顶点出发能够到达的顶点，从而省去了对不相连顶点的存储空间。然而，这还不够。对于有向图而言，图中有效信息除了从顶点“指出去”的信息，还包括从别的顶点“指进来”的信息。这里的“指出去”和“指进来”可以用出度和入度来表示。

> 入度：有向图的某个顶点作为终点的次数和。
>
> 出度：有向图的某个顶点作为起点的次数和。

由此看出，在对有向图进行表示时，邻接表只能求出图的出度，而无法求出入度(应该可以遍历除了当前出度链表的其他链表，查询其他链表中是否有当前节点，不过这样的成本太高了）。这个问题很好解决，那就是增加一个表用来存储能够到达某个顶点的相邻顶点。这个表称作逆邻接表。

#### 逆邻接表

逆邻接表与邻接表结构类似，只不过图的顶点链接着能够到达该顶点的相邻顶点。也就是说，邻接表时顺着图中的箭头寻找相邻顶点，而逆邻接表时逆着图中的箭头寻找相邻顶点。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKqLv4dnM8ibblfCzdabl2RV52ZCiaZlHGobcxp6ydoOzXxalor9mDN8dA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

邻接表和逆邻接表的共同使用下，就能够把一个完整的有向图结构进行表示。可以发现，邻接表和逆邻接表实际上有一部分数据时重合的，因此可以将两个表合二为一，从而得到了所谓的十字链表。

#### 十字链表

十字链表似乎很简单，只需要通过相同的顶点分别链向以该顶点为终点和起点的相邻顶点即可。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKORBfPSQiblo2CLzrahdcNeqMvzm93kKiaybG4iaCD2TKH4vs8IkxRmppw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

但这并不是最优的表示方式。虽然这样的方式共用了中间的顶点存储空间，但是邻接表和逆邻接表的链表节点中重复出现的顶点并没有得到重复利用，反而是进行了再次存储。因此，上图的表示方式还可以进行进一步优化。

十字链表优化后，可通过扩展的顶点结构和边结构来进行正逆邻接表的存储：（下面的弧头可看作是边的箭头那端，弧尾可看作是边的圆点那端）

> **data**：用于存储该顶点中的数据；
>
> **firstin指针**：用于连接以当前顶点为弧头的其他顶点构成的链表，即从别的顶点指进来的顶点；
>
> **firstout指针**：用于连接以当前顶点为弧尾的其他顶点构成的链表，即从该顶点指出去的顶点；

边结构通过存储两个顶点来确定一条边，同时通过分别代表这两个顶点的指针来与相邻顶点进行链接：

> **tailvex**：用于存储作为弧尾的顶点的编号；
>
> **headvex**：用于存储作为弧头的顶点的编号；
>
> **headlink** **指针**：用于链接下一个存储作为弧头的顶点的节点；
>
> **taillink** **指针**：用于链接下一个存储作为弧尾的顶点的节点；

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TdGLaSU675gwBawZm4AtKbyUUqEHRqCKsPXqiaIeicYpPEBPrs3jVRs795hLAUQ44748sjFLWhnsZ7fGL2OkicJ6g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

以上图为例子，对于顶点A而言，其作为起点能够到达顶点E。因此在邻接表中顶点A要通过边`AE`（即边04）指向顶点E，顶点A的`firstout`指针需要指向边04的`tailvex`。同时，从B出发能够到达A，所以在逆邻接表中顶点A要通过边`AB`（即边10）指向B，顶点A的`firstin`指针需要指向边10的弧头，即`headlink`指针。依次类推。

十字链表采用了一种看起来比较繁乱的方式对边的方向性进行了表示，能够在尽可能降低存储空间的情况下增加指针保留顶点之间的方向性。具体的操作可能一时间不好弄懂，建议多看几次上图，弄清指针指向的意义，明白正向和逆向邻接表的表示。

##  10 总结

数据结构博大精深，没有高等数学的讳莫如深，也没有量子力学的玄乎其神，但是其在计算机科学的各个领域都具有强大的力量。本文试图采用图解的方式对九种数据结构进行理论上的介绍，但是其实这都是不够的。

即便是简单的数组、栈、队列等结构，在实际使用以及底层实现上都会有许多优化设计以及使用技巧，这意味着还需要真正把它们灵活的用起来，才能够算是真正意义上的熟悉和精通。但是本文可以作为常见数据结构的一个总结，当你对某些结构有些淡忘的时候，不妨重新回来看看。

---

# 哈希表详解

**哈希表**不论在**刷题**，还是**实际业务代码**中，都是应用极其**广泛**的一种数据结构，**出场率**特别高。所以今天我们一起把哈希表的内些事给整明白吧，当场KO它，文章框架如下。

![图片](https://mmbiz.qpic.cn/mmbiz_png/ClAkUIOhotXcK4UNqibN3w7gtOpGaWSZmpGaZLRoeZiciawx7R6ZZbucme7HdVvS04xVCRYpZkj263ysl57cKibdLw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 散列表查找步骤

散列表，最有用的基本数据结构之一。是根据关键码的值直接进行访问的数据结构，散列表的实现常常叫做**散列（hasing）**。散列是一种用于以**常数平均时间**执行插入、删除和查找的技术，下面我们来看一下散列过程。

我们的整个散列过程主要分为两步

（1）通过**散列函数**计算记录的散列地址，并按此**散列地址**存储该记录。就好比麻辣鱼，我们就让它在川菜区，糖醋鱼，我们就让它在鲁菜区。但是我们需要注意的是，无论什么记录我们都需要用**同一个散列函数**计算地址，然后再存储。

（2)当我们查找时，我们通过**同样的散列函数**计算记录的散列地址，按此散列地址访问该记录。因为我们存和取的时候用的都是一个散列函数，因此结果肯定相同。

刚才我们在散列过程中提到了散列函数，那么散列函数是什么呢？

我们假设某个函数为 **f**，使得 **存储位置 = f (key)** 那样我们就能通过查找关键字**不需要比较**就可获得需要的记录的存储位置。这种存储技术被称为散列技术。散列技术是在通过记录的存储位置和它的关键字之间建立一个确定的对应关系 f ,使得每个关键字 key 都对应一个存储位置 f(key)。见下图

![图片](https://mmbiz.qpic.cn/mmbiz_png/ClAkUIOhotUYMyvfTOEhJNxsiaO9L8gmLuXzlzJicDeBWYASicHCmMl4sDTibia6nZr1fUuLribHms7ydG7iaosaSkFqA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

这里的 **f** 就是我们所说的散列函数（哈希）函数。我们利用散列技术将记录存储在一块连续的存储空间中，这块连续存储空间就是我们本文的主人公------散列(哈希)

上图为我们描述了用散列函数将关键字映射到散列表，但是大家有没有考虑到这种情况，那就是将关键字映射到同一个槽中的情况，即 f(k4) = f(k3) 时。这种情况我们将其称之为**冲突**，k3 和 k4 则被称之为散列函数 **f** 的**同义词**，如果产生这种情况，则会让我们查找错误。幸运的是我们能找到有效的方法解决冲突。

首先我们可以对哈希函数下手，我们可以精心设计哈希函数，让其尽可能少的产生冲突，所以我们创建哈希函数时应遵循以下规则

（1）**必须是一致的**，假设你输入辣子鸡丁时得到的是在看，那么每次输入辣子鸡丁时，得到的也必须为在看。如果不是这样，散列表将毫无用处。

（2）**计算简单**，假设我们设计了一个算法，可以保证所有关键字都不会冲突，但是这个算法计算复杂，会耗费很多时间，这样的话就大大降低了查找效率，反而得不偿失。所以咱们**散列函数的计算时间不应该超过其他查找技术与关键字的比较时间**，不然的话我们干嘛不使用其他查找技术呢?

（3）**散列地址分布均匀**我们刚才说了冲突的带来的问题，所以我们最好的办法就是让**散列地址尽量均匀分布在存储空间中**，这样即保证空间的有效利用，又减少了处理冲突而消耗的时间。

现在我们已经对散列表，散列函数等知识有所了解啦，那么我们来看几种常用的散列函数构造方法。这些方法的共同点为都是将原来的数字按某种规律变成了另一个数字。所以是很容易理解的。

### 散列函数构造方法

#### 直接定址法

如果我们对盈利为0-9的菜品设计哈希表，我们则直接可以根据作为地址，则 **f(key) = key**;

即下面这种情况。

![图片](https://mmbiz.qpic.cn/mmbiz_png/ClAkUIOhotXcK4UNqibN3w7gtOpGaWSZmkS3ppiaQb5rCog09cFXp8osm2Go3M4KWjiaPVmrZso4gBLjm5reXFW1w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

有没有感觉上面的图很熟悉，没错我们经常用的数组其实就是一张哈希表，关键码就是数组的索引下标，然后我们通过下标直接访问数组中的元素。

另外我们假设每道菜的成本为50块，那我们还可以根据盈利+成本来作为地址，那么则 f(key) = key + 50。也就是说我们可以根据线性函数值作为散列地址。

**f(key)  =  a \* key + b**   **a,b均为常数**

优点：简单、均匀、无冲突。

应用场景：需要事先知道关键字的分布情况，适合查找表较小且连续的情况

#### 数字分析法

该方法也是十分简单的方法，就是分析我们的关键字，取其中一段，或对其位移，叠加，用作地址。比如我们的学号，前 6 位都是一样的，但是后面 3 位都不相同，我们则可以用学号作为键，后面的 3 位做为我们的散列地址。如果我们这样还是容易产生冲突，则可以对抽取数字再进行处理。我们的目的只有一个，提供一个散列函数将关键字合理的分配到散列表的各位置。这里我们提到了一种新的方式抽取，这也是在散列函数中经常用到的手段。

![图片](https://mmbiz.qpic.cn/mmbiz_png/ClAkUIOhotUYMyvfTOEhJNxsiaO9L8gmL9EicHHGfZpnwIPyQC6ut6D7IrLU0OKicKBBRxVNCkm1xepssRpWeYUNw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

优点：简单、均匀、适用于关键字位数较大的情况

应用场景：关键字位数较大，知道关键字分布情况且关键字的若干位较均匀

#### 折叠法 

其实这个方法也很简单，也是处理我们的关键字然后用作我们的散列地址，主要思路是将关键字从左到右分割成位数相等的几部分，然后叠加求和，并按散列表表长，取后几位作为散列地址。

比如我们的关键字是123456789，则我们分为三部分 123 ，456 ，789 然后将其相加得 1368 然后我们再取其后三位 368 作为我们的散列地址。

优点：事先不需要知道关键字情况

应用场景：适合关键字位数较多的情况

#### 除法散列法

在用来设计散列函数的除法散列法中，通过取 key 除以 p 的余数，将关键字映射到 p 个槽中的某一个上，对于散列表长度为 m 的散列函数公式为

**f(k) = k mod p  (p <= m)**

例如，如果散列表长度为 12，即 m = 12 ，我们的参数 p 也设为12，那 k = 100时 f(k) = 100 % 12 = 4

由于只需要做一次除法操作，所以除法散列法是非常快的。

由上面的公式可以看出，该方法的重点在于 p 的取值，如果 p 值选的不好，就可能会容易产生同义词。见下面这种情况。我们哈希表长度为6，我们选择6为p值，则有可能产生这种情况，所有关键字都得到了0这个地址数。

![图片](https://mmbiz.qpic.cn/mmbiz_png/ClAkUIOhotUYMyvfTOEhJNxsiaO9L8gmLBrByS9bHuTMMkyXQNZGd7ys1UlrcicPyM0IdVZZAByjtDEZfEhKVPqg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

那我们在选用除法散列法时选取 p 值时应该遵循怎样的规则呢？

- m 不应为 2 的幂，因为如果 m = 2^p^ ，则 f(k) 就是 k 的 p 个最低位数字。例 12 % 8 = 4 ，12的二进制表示位1100，后三位为100。
- 若散列表长为 m ,通常 p 为 小于或等于表长（最好接近m）的最小质数或不包含小于 20 质因子的合数。

> **合数：**合数是指在大于1的整数中除了能被1和本身整除外，还能被其他数（0除外）整除的数。
>
> **质因子**：质因子（或质因数）在数论里是指能整除给定正整数的质数。

![图片](https://mmbiz.qpic.cn/mmbiz_svg/0nn3FBrD9a2LeIMgric7KKS1SmA5CITt6nqzLhSqN9yoglpGj0icAOrnVW5ZuwdLIwLHDxXIX4TcBiaOMvg2g4SH1cM88f2liaJF/640?wx_fmt=svg&wxfrom=5&wx_lazy=1&wx_co=1)

注：这里的2，3，5为质因子

还是上面的例子，我们根据规则选择 5 为 p 值，我们再来看。这时我们发现只有 6 和 36 冲突，相对来说就好了很多。

![图片](https://mmbiz.qpic.cn/mmbiz_png/ClAkUIOhotUYMyvfTOEhJNxsiaO9L8gmL0847qYsXnj5NrFA9Yl04Jib93fiaphUqqCzVQ3tlOV62g4HaCvu7rcYQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

优点：计算效率高，灵活

应用场景：不知道关键字分布情况

#### 乘法散列法

构造散列函数的乘法散列法主要包含两个步骤

- 用关键字 k 乘上常数 A(0 < A < 1)，并提取 k A 的小数部分
- 用 m 乘以这个值，再向下取整

散列函数为

**f (k) = ⌊ m(kA mod 1) ⌋**

`f(k) = ⌊ m(kA mod ⌊kA⌋) ⌋`

这里的 **kA mod 1** 的含义是取 keyA 的小数部分，即 **kA - ⌊kA⌋** 。

优点：对 m 的选择不是特别关键一般选择它为 2 的某个幂次（m = 2 ^p^,p为某个整数）

应用场景：不知道关键字情况

#### 平方取中法

这个方法就比较简单了，假设关键字是 321，那么他的平方就是 103041，再抽取中间的 3 位就是 030 或 304 用作散列地址。再比如关键字是 1234  那么它的平方就是 1522756 ，抽取中间 3 位就是 227 用作散列地址.

优点：灵活，适用范围广泛

适用场景：不知道关键字分布，而位数又不是很大的情况。

#### 随机数法

故名思意，取关键字的随机函数值为它的散列地址。也就是 **f(key) = random(key)**。这里的random是随机函数。（具体解析见随机探测法）

适用场景：关键字的长度不等时

上面我们的例子都是通过数字进行举例，那么如果是字符串可不可以作为键呢？当然也是可以的，各种各样的符号我们都可以转换成某种数字来对待，比如我们经常接触的ASCII 码，所以是同样适用的。

以上就是常用的散列函数构造方法，其实他们的中心思想是一致的，将关键字经过加工处理之后变成另外一个数字，而这个数字就是我们的存储位置，是不是有一种间谍传递情报的感觉。

一个好的哈希函数可以帮助我们尽可能少的产生冲突，但是也不能完全避免产生冲突，那么遇到冲突时应该怎么做呢？下面给大家带来几种常用的处理散列冲突的方法。

### 处理散列冲突的方法

我们在使用 hash 函数之后发现关键字 key1 不等于 key2 ，但是 f(key1) = f(key2)，即有冲突，那么该怎么办呢？不急我们慢慢往下看。

#### 开放地址法

**开放地址法**就是一旦发生冲突，就去寻找下一个空的散列地址，只要列表足够大，空的散列地址总能找到，并将记录存入，为了使用开放寻址法插入一个元素，需要连续地检查散列表，或称为**探查**，我们常用的有**线性探测，二次探测，随机探测**。

##### 线性探测法

下面我们先来看一下线性探测，公式：

![图片](https://mmbiz.qpic.cn/mmbiz_png/ClAkUIOhotUYMyvfTOEhJNxsiaO9L8gmLuKUtqvE0vl5g1tA4TpOPv1LyeBzL71FfMNy8Sy65LxY9GriaNbhzFGA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

我们来看一个例子，我们的关键字集合为{12，67，56，16，25，37，22，29，15，47，48，21}，表长为12，我们再用散列函数 **f(key) =  key mod 12。**

我们求出每个 key 的 f(key)见下表

![图片](https://mmbiz.qpic.cn/mmbiz_png/ClAkUIOhotUYMyvfTOEhJNxsiaO9L8gmLuHIiaLV64YHFQsXGCCCZBwHIPF9kKibzheOKaKHdiaR1iagu15HVZBiaUuQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

我们查看上表发现，前五位的 **f(key)** 都不相同，即没有冲突，可以直接存入，但是到了第六位 **f(37) = f(25) = 1**,那我们就需要利用上面的公式 **f(37)  = f (f(37) + 1 ) mod 12 = 2**。下面我们看一下将上面的所有数存入哈希表是什么情况吧。

注：蓝色为计算哈希值，红色为存入哈希表

![图片](https://mmbiz.qpic.cn/mmbiz_png/ClAkUIOhotUYMyvfTOEhJNxsiaO9L8gmLOcc5q0RjwBGuZic3ZdHGHlZo69zDqVl1cBCGia1M5Z7S2M4GjsOklSYA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



另外我们在解决冲突的时候，会遇到 48 和 37 虽然不是同义词，却争夺一个地址的情况，我们称其为**堆积**。因为堆积使得我们需要不断的处理冲突，插入和查找效率都会大大降低。

通过上面的视频我们应该了解了线性探测的执行过程了，那么我们考虑一下这种情况，若是我们的最后一位不为21，为 34 时会有什么事情发生呢？

![图片](https://mmbiz.qpic.cn/mmbiz_png/ClAkUIOhotUYMyvfTOEhJNxsiaO9L8gmLUTErCmmVU08eHrvZQibFAsOtpwlFuQUFTv9iahdM5KObRCDEpl4njS1A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



此时他第一次会落在下标为 10 的位置，那么如果继续使用线性探测的话，则需要通过不断取余后得到结果，数据量小还好，要是很大的话那也太慢了吧，但是明明他的前面就有一个空房间呀，如果向前移动只需移动一次即可。不要着急，前辈们已经帮我们想好了解决方法

##### 二次探测法

其实理解了我们的上个例子之后，这个一下就能整明白了，根本不用费脑子，这个方法就是更改了一下di的取值

![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

注：这里的是 -1^2  为负值 而不是 （-1)^2

所以对于我们的34来说，当di = -1时，就可以找到空位置了。

![图片](https://mmbiz.qpic.cn/mmbiz_png/ClAkUIOhotUYMyvfTOEhJNxsiaO9L8gmLkP5HOsvlVSwEicAMLD10pQKmsuMt9Oibjsb7QkTyAqYEbj7uVBL0ibBPQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

二次探测法的目的就是为了不让关键字聚集在某一块区域。另外还有一种有趣的方法，位移量采用随机函数计算得到，接着往下看吧.

##### 随机探测法

大家看到这是不又有新问题了，刚才我们在散列函数构造规则的第一条中说

> （1）**必须是一致的**，假设你输入辣子鸡丁时得到的是**在看**，那么每次输入辣子鸡丁时，得到的也必须为**在看**。如果不是这样，散列表将毫无用处。

咦？怎么又是**在看**哈哈，那么问题来了，我们使用随机数作为他的偏移量，那么我们查找的时候岂不是查不到了？因为我们 di 是随机生成的呀，这里的随机其实是伪随机数，伪随机数含义为，我们设置**随机种子**相同，则不断调用随机函数可以生成**不会重复的数列**，我们在查找时，**用同样的随机种子**，**它每次得到的数列是相同的**，那么相同的 di 就能得到**相同的散列地址**。

> 随机种子（Random Seed）是计算机专业术语，一种以随机数作为对象的以真随机数（种子）为初始条件的随机数。一般计算机的随机数都是伪随机数，以一个真随机数（种子）作为初始条件，然后用一定的算法不停迭代产生随机数

![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)



![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

通过上面的测试是不是一下就秒懂啦，使用相同的随机种子，生成的数列是相同的。所以为什么我们可以使用随机数作为它的偏移量。

下面我们再来看一下其他的函数处理散列冲突的方法

#### 再哈希法

这个方法其实也特别简单，利用不同的哈希函数再求得一个哈希地址，直到不出现冲突为止。

> **f,(key) = RH,( key )   (i = 1,2,3,4…..k)**

这里的RH,就是不同的散列函数，你可以把我们之前说过的那些散列函数都用上，每当发生冲突时就换一个散列函数，相信总有一个能够解决冲突的。这种方法能使关键字不产生聚集，但是代价就是增加了计算时间。是不是很简单啊。

#### 链地址法

上面我们都是遇到冲突之后，就换地方。那么我们有没有不换地方的办法呢？那就是我们现在说的链地址法。

还记得我们说过的同义词吗？就是 key 不同 f(key) 相同的情况，我们将这些同义词存储在一个单链表中，这种表叫做同义词子表，散列表中只存储同义词子表的头指针。我们还是用刚才的例子，关键字集合为{12，67，56，16，25，37，22，29，15，47，48，21}，表长为12，我们再用散列函数 **f(key) =  key mod 12。**我们用了链地址法之后就再也不存在冲突了，无论有多少冲突，我们只需在同义词子表中添加结点即可。下面我们看下链地址法的存储情况。

![图片](https://mmbiz.qpic.cn/mmbiz_png/ClAkUIOhotUYMyvfTOEhJNxsiaO9L8gmLG7ngOia8ljB0ia9Pn4jgG1u42pic8VmxDam7CPF4Jic24icxB0SjMeIicOJw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

链地址法虽然能够不产生冲突，但是也带来了查找时需要遍历单链表的性能消耗，有得必有失嘛。

#### 公共溢出区法

公共溢出区法，这也是很好理解的，你不是冲突吗？那冲突的各位我先给你安排个地方呆着，这样你就有地方住了。我们为所有冲突的关键字建立了一个公共的溢出区来存放。

![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

那么我们怎么进行查找呢？我们首先通过散列函数计算出散列地址后，先于基本表对比，如果不相等再到溢出表去顺序查找。这种解决冲突的方法，对于冲突很少的情况性能还是非常高的。

### 散列表查找算法(线性探测法)

下面我们来看一下散列表查找算法的实现

首先需要定义散列列表的结构以及一些相关常数，其中elem代表散列表数据存储数组，count代表的是当前插入元素个数，size代表哈希表容量，NULLKEY散列表初始值，然后我们如果查找成功就返回索引，如果不存在该元素就返回元素不存在。

我们将哈希表初始化，为数组元素赋初值。



![图片](https://mmbiz.qpic.cn/mmbiz_png/ClAkUIOhotXcK4UNqibN3w7gtOpGaWSZmdHpBVwZabnrSfqbicXjx7HV27KZPQ1QZ9ocMlSSf1GViaia3icPhzl0YSA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



插入操作的具体步骤：

（1）通过哈希函数(除法散列法)，将key转化为数组下标

（2）如果该下标中没有元素，则插入，否则说明有冲突，则利用线性探测法处理冲突。详细步骤见注释



![图片](https://mmbiz.qpic.cn/mmbiz_png/ClAkUIOhotXcK4UNqibN3w7gtOpGaWSZmCWJIsHbfzXicKjGz0y8kibwictqyVwQMW99OBp7oP2Qy0TI4OEHZGrGFA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

查找操作的具体步骤：

（1）通过哈希函数（同插入时一样），将key转化成数组下标

（2）通过数组下标找到key值，如果key一致，则查找成功，否则利用线性探测法继续查找。



![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

下面我们来看一下完整代码

![图片](https://mmbiz.qpic.cn/mmbiz_png/ClAkUIOhotXcK4UNqibN3w7gtOpGaWSZmaXoPicMicMc6d1iaewODHhjZBGY8xXgMX69DQUeVxEpItnRuJJdnEoGdQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 散列表性能分析

如果没有冲突的话，散列查找是我们查找中效率最高的，时间复杂度为O(1),但是没有冲突的情况是一种理想情况，那么散列查找的平均查找长度取决于哪些方面呢？

**1.散列函数是否均匀**

我们在上文说到，可以通过设计散列函数减少冲突，但是由于不同的散列函数对一组关键字产生冲突可能性是相同的，因此我们可以不考虑它对平均查找长度的影响。

**2.处理冲突的方法**

相同关键字，相同散列函数，不同处理冲突方式，会使平均查找长度不同，比如我们线性探测有时会堆积，则不如二次探测法好，因为链地址法处理冲突时不会产生任何堆积，因而具有最佳的平均查找性能

**3.散列表的装填因子**

本来想在上文中提到装填因子的，但是后来发现即使没有说明也不影响我们对哈希表的理解，下面我们来看一下装填因子的总结

> 装填因子 α  =  填入表中的记录数  /  散列表长度

散列因子则代表着散列表的装满程度，表中记录越多，α就越大，产生冲突的概率就越大。我们上面提到的例子中 表的长度为12，填入记录数为6，那么此时的 α = 6  / 12 = 0.5  所以说当我们的 α 比较大时再填入元素那么产生冲突的可能性就非常大了。所以说散列表的平均查找长度取决于装填因子，而不是取决于记录数。所以说我们需要做的就是选择一个合适的装填因子以便将平均查找长度限定在一个范围之内。